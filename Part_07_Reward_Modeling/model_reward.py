"""

ðŸŽ¯ What is a Reward Model?
A Reward Model (RM) is a neural network that learns to judge how good a modelâ€™s output is â€” essentially, it assigns a score (a reward) to text generated by a language model.
You can think of it as the â€œcriticâ€ in a teacherâ€“student setup:
    1. The LLM (policy model) generates possible answers.
    2. The reward model evaluates them â€” giving higher scores to better, more human-like responses.

ðŸ§© How It Works â€” Step by Step
1. Collect Human Preferences
Humans are shown multiple model responses (e.g., 2 answers to the same prompt):
    - Prompt: "Explain photosynthesis."
    - Response A: A clear, correct explanation.
    - Response B: A vague or wrong one.
Humans mark which one is better. These become preference pairs: (A preferred over B)

ðŸ§  Train the Reward Model
The Reward Model takes the prompt + response as input and outputs a scalar score 'r'.
For a pair (r_pos, r_neg):
    r_pos = reward for the preferred response
    r_neg = reward for the non-preferred one
Then we train using a pairwise loss, such as:
    ðŸ§® Bradleyâ€“Terry Loss:
        ð¿ = âˆ’log(Ïƒ(r_pos âˆ’ r_neg))
This means:
    If r_pos > r_neg, loss is small âœ…
    If r_pos < r_neg, loss is large âŒ
The model thus learns to give higher scores to human-preferred responses.

ðŸ“ˆ Why Itâ€™s Important

âœ… Aligns the model with human values and preferences
âœ… Enables safe and polite LLM behavior
âœ… Foundation for DPO, PPO, RLHF, and RLAIF training
âœ… Can be reused for RLAIF (AI Feedback) or constitutional AI

"""


from __future__ import annotations
import torch, torch.nn as nn



class RewardModel(nn.Module):
    """
    Transformer encoder â†’ pooled representation â†’ scalar reward.
    Bidirectional encoder is fine for reward modeling (not used for generation).
    """
    def __init__(self, vocab_size: int, block_size: int, n_layer: int = 4, n_head: int = 4, n_embd: int = 256, dropout: float = 0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.block_size = block_size
        self.tok_emb = nn.Embedding(vocab_size, n_embd)
        self.pos_emb = nn.Embedding(block_size, n_embd)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=n_embd,
            nhead=n_head,
            dim_feedforward=4 * n_embd,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layer)
        self.ln = nn.LayerNorm(n_embd)
        self.head = nn.Linear(n_embd, 1)
    
    
    def forward(self, x: torch.Tensor):
        B, T = x.shape
        pos = torch.arange(T, device=x.device).unsqueeze(0)
        h = self.tok_emb(x) + self.pos_emb(pos)
        pad_mask = (x == 2)
        h = self.encoder(h, src_key_padding_mask=pad_mask)
        h = self.ln(h)
        
        # masked mean pool over tokens (ignoring pads)
        mask = (~pad_mask).float().unsqueeze(-1)
        h_sum = (h * mask).sum(dim=1)
        len_ = mask.sum(dim=1).clamp_min(1.0)
        pooled = h_sum / len_
        r = self.head(pooled).squeeze(-1)  # (B,)
        
        return r

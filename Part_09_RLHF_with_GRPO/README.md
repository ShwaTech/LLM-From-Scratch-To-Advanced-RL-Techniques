# **RLHF with GRPO** (Reinforcement Learning from Human Feedback using Group Relative Policy Optimization) ðŸ‘‡

## ðŸš€ Extremly Important Terms for ( RL + LLMs )

| Term | Definition |
|------|-------------|
| **State (sâ‚œ)** | The current text context â€” prompt plus tokens generated so far. |
| **Action (aâ‚œ)** | The next token the model chooses to emit. |
| **Stateâ€“action (sâ‚œ, aâ‚œ)** | A specific context and the specific next token chosen in it. |
| **Policy (Ï€Î¸)** | The LLMâ€™s probability distribution over next tokens given the context. |
| **Value (V(sâ‚œ))** | Predicted total future preference/reward from this context if we keep sampling from Ï€Î¸. |
| **Reward (râ‚œ)** | A scalar score from a reward model or rule (only at the end of the answer). |
| **Returns (Gâ‚œ)** | Discounted sum of future rewards for the rest of the generation. |
| **Q-value (Q(sâ‚œ, aâ‚œ))** | Expected return if we emit that token now and then continue with Ï€Î¸. |
| **Advantage (A = Q â€“ V)** | How much better that token is than the modelâ€™s average continuation at this context. |
| **KL divergence (Dâ‚–â‚—(Ï€Î¸ âˆ¥ Ï€_ref))** | Penalty measuring how far the current token distribution drifts from a frozen SFT/reference model at the same context. |
| **Policy vs Ref** | **Policy:** the RL-tuned, updating LLM. **Ref:** the frozen SFT model used for safety and KL regularization. |

## ðŸ§  What is GRPO?

GRPO stands for **Group Relative Policy Optimization**.
Itâ€™s a variant of policy-optimization used in RLHF for large language models (LLMs), proposed to replace or augment standard methods like Proximal Policy Optimization (PPO).

**Key characteristics:**

* It **eliminates the need for a learned â€œvalue functionâ€ (critic)**, reducing complexity and memory requirements.
* Instead, it uses **group-based sampling**: for each prompt/state, multiple answers (actions) are generated, each scored by a reward model. The *average reward* across the group becomes a baseline, and each answerâ€™s advantage is its reward minus that baseline.
* The policy update then favors answers that are *better than the group average* and disfavours those that are worse.

## ðŸ” Why GRPO? (How it improves over standard PPO in RLHF)

| Challenge in PPO for LLMs                                                    | How GRPO addresses it                                                                                                                |
| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| Need to learn a separate value network (critic) â†’ high memory & compute cost | GRPO removes the critic and uses the group average as a baseline instead.                                              |
| High variance in reward signals, unstable advantage estimates                | By generating multiple outputs per prompt and using the group mean, GRPO stabilizes the baseline and reduces variance. |
| Large models + long outputs â†’ huge cost for RL fineâ€tuning                   | GRPO cuts resource needs, making RLHF more accessible even for smaller setups.                                       |

## ðŸ› ï¸ How GRPO Works â€” Step by Step

1. **Prompt sampling:** Choose a batch of prompts (states) from your dataset.
2. **Generate a group of responses per prompt:** For each prompt, the policy model generates multiple candidate answers (e.g., 8, 16, 64).
3. **Reward scoring:** A reward model (or feedback function) assigns a reward to each response.
4. **Compute baseline:** For each prompt, compute the *average reward* of its group of responses:
    RË‰ = 1/k * âˆ‘â€‹(1â†’N) RÎº
5. **Compute advantage for each response:**
   AÎº â€‹= RÎº â€‹âˆ’ RË‰
   So responses above average get positive advantage; below average get negative.
6. **Policy update:** Use these advantages in a surrogate objective (similar to PPO) to update the policy. Often also a KLâ€penalty or regularization term ensures the updated policy doesnâ€™t drift too far from a reference policy.
7. **Repeat:** Continue with new batches of prompts/responses.

## ðŸŽ¯ GRPO in the RLHF Pipeline

Here is how it fits into RLHF:

* **Pretraining:** The base language model learns from large text corpora.
* **Supervised Fine-Tuning (SFT):** The model is finetuned on prompt-response pairs so it follows instructions.
* **Reward Modeling (RM):** A separate model learns to score responses based on human preferences or other criteria.
* **GRPO (instead of PPO):** The SFT model becomes the *policy*, uses the RM for rewards, and is finetuned via the GRPO algorithm to produce higher-reward (more human-aligned) outputs.

## ðŸ§¾ Summary Table

| Component                     | Role in GRPO                                         | Why it matters                                     |
| ----------------------------- | ---------------------------------------------------- | -------------------------------------------------- |
| Policy model ( Ï€Î¸â€‹ )   | The LLM being optimized                              | We want it to produce better answers.              |
| Reward model                  | Scores each generated response                       | Provides the â€œgood vs badâ€ signal.                 |
| Group of responses per prompt | Multiple answers generated per prompt                | Enables using group baseline instead of valueâ€net. |
| Baseline = group mean reward  | Used to compute advantage                            | Simplifies advantage estimation.                   |
| Advantage ( A = R âˆ’ RË‰ ) | Drives updates: positiveâ†’increase, negativeâ†’decrease | Focuses on relative improvement.                   |
| KL / regularization           | Keeps policy from diverging too far                  | Ensures stability and safety.                      |

## ðŸ“Œ In One Sentence

> **GRPO is a policy-optimization algorithm for RLHF that generates multiple responses per prompt, uses the group average as a baseline instead of a learned critic, and updates the policy to favour responses that score above that baseline â€” making RLHF more efficient and stable for large language models.**

"""

ðŸ§  1. What Problem Does KV Cache Solve?

In autoregressive LLMs (like GPT, LLaMA, etc.), the model generates one token at a time.
At each new token, it performs self-attention over all previously generated tokens.

Without optimization:
    For token 1 â†’ attends to nothing (just itself).
    For token 2 â†’ attends to token 1.
    For token 3 â†’ attends to tokens 1 and 2.
    ...
    For token n â†’ attends to all 1...nâ€“1.

That means for n tokens, total work â‰ˆ O(nÂ²).
Worse: we recompute attention for all past tokens every step!
Thatâ€™s extremely inefficient for long contexts.

âš™ï¸ 2. What is KV Cache?
During inference, we donâ€™t need to recompute past attention.
The keys (K) and values (V) from previous tokens never change â€” we can just store them.
So, we cache them once and reuse them on every new token generation.
Hence: KV Cache = stored Keys and Values from previous steps.

ðŸ“Š 3. How It Works (Step-by-Step)
Letâ€™s say weâ€™re generating text with a transformer.
Step 1: Input: "Once"
    Compute query (Qâ‚), key (Kâ‚), and value (Vâ‚).
    Store Kâ‚ and Vâ‚ in cache.
Step 2: Next input: "upon"
    Compute new Qâ‚‚ (for "upon").
    Load old Kâ‚, Vâ‚ from cache.
    Concatenate them with new Kâ‚‚, Vâ‚‚:
        K_total = [Kâ‚, Kâ‚‚]
        V_total = [Vâ‚, Vâ‚‚]
    Compute attention(Qâ‚‚, K_total, V_total)
    Store new Kâ‚‚, Vâ‚‚ to cache.
Step 3: Repeat this for each new token:
    Queries are computed fresh.
    Keys and values are reused.

âœ… That reduces redundant computation.

"""


from __future__ import annotations
import torch
from dataclasses import dataclass


@dataclass
class KVCache:
    k: torch.Tensor  # (B, H, T, D)
    v: torch.Tensor  # (B, H, T, D)
    
    @property
    def T(self):
        return self.k.size(2)


class RollingKV:
    """
    Rolling buffer with optional attention sink.
    Keeps first `sink` tokens + last `window` tokens.
    """
    def __init__(self, window: int, sink: int = 0):
        self.window = window
        self.sink = sink
        self.k = None
        self.v = None
    
    def step(self, k_new: torch.Tensor, v_new: torch.Tensor):
        if self.k is None:
            self.k, self.v = k_new, v_new
        else:
            self.k = torch.cat([self.k, k_new], dim=2)
            self.v = torch.cat([self.v, v_new], dim=2)
        
        # crop
        if self.k.size(2) > self.window + self.sink:
            sink_part = self.k[:, :, :self.sink, :]
            sink_val  = self.v[:, :, :self.sink, :]
            tail_k = self.k[:, :, -self.window:, :]
            tail_v = self.v[:, :, -self.window:, :]
            self.k = torch.cat([sink_part, tail_k], dim=2)
            self.v = torch.cat([sink_val, tail_v], dim=2)
        
        return self.k, self.v

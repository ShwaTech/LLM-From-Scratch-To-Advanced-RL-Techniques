# **RLHF with PPO** (Reinforcement Learning from Human Feedback using Proximal Policy Optimization) ðŸ‘‡

## ðŸš€ Extremly Important Terms for ( RL + LLMs )

| Term | Definition |
|------|-------------|
| **State (sâ‚œ)** | The current text context â€” prompt plus tokens generated so far. |
| **Action (aâ‚œ)** | The next token the model chooses to emit. |
| **Stateâ€“action (sâ‚œ, aâ‚œ)** | A specific context and the specific next token chosen in it. |
| **Policy (Ï€Î¸)** | The LLMâ€™s probability distribution over next tokens given the context. |
| **Value (V(sâ‚œ))** | Predicted total future preference/reward from this context if we keep sampling from Ï€Î¸. |
| **Reward (râ‚œ)** | A scalar score from a reward model or rule (only at the end of the answer). |
| **Returns (Gâ‚œ)** | Discounted sum of future rewards for the rest of the generation. |
| **Q-value (Q(sâ‚œ, aâ‚œ))** | Expected return if we emit that token now and then continue with Ï€Î¸. |
| **Advantage (A = Q â€“ V)** | How much better that token is than the modelâ€™s average continuation at this context. |
| **KL divergence (Dâ‚–â‚—(Ï€Î¸ âˆ¥ Ï€_ref))** | Penalty measuring how far the current token distribution drifts from a frozen SFT/reference model at the same context. |
| **Policy vs Ref** | **Policy:** the RL-tuned, updating LLM. **Ref:** the frozen SFT model used for safety and KL regularization. |

## ðŸ§  What is RLHF?

**Reinforcement Learning from Human Feedback (RLHF)** is a method to make large language models (LLMs) behave in ways that **humans prefer** â€” polite, helpful, safe, and aligned with instructions.

It does this by **fine-tuning a pretrained model** using **human preference data** instead of just next-token prediction.

## âš™ï¸ RLHF = 3 Main Stages

| Stage                                 | Description                                                                              | Output                           |
| ------------------------------------- | ---------------------------------------------------------------------------------------- | -------------------------------- |
| 1ï¸âƒ£ **Supervised Fine-Tuning (SFT)**  | Train the model on high-quality instruction data (prompt â†’ correct response).            | Base policy (good starter model) |
| 2ï¸âƒ£ **Reward Modeling (RM)**          | Train a model to score responses by human preference.                                    | Reward model (judge)             |
| 3ï¸âƒ£ **Reinforcement Learning (RLHF)** | Use RL (PPO) to make the base model maximize reward while staying close to SFT behavior. | Aligned model                    |

## ðŸŽ¯ Goal of PPO Stage (Stage 3)

We now have:

* ðŸ§© **Policy model (Ï€Î¸)** â†’ The LLM weâ€™re training.
* ðŸ§© **Reward model (RÏ•)** â†’ The â€œcriticâ€ that scores outputs.
* ðŸ§© **Reference model (Ï€_ref)** â†’ A frozen copy of the SFT model (used for regularization).

We want to update the policy (LLM) so it:

1. Produces answers that get **higher rewards** (good quality).
2. Stays **close to the original model** (to avoid going off track).

## ðŸ’¡ PPO: Proximal Policy Optimization

PPO is a **safe and stable reinforcement learning algorithm**.
It updates the model *just enough* each step â€” not too much â€” to prevent instability.

### The PPO idea

> Donâ€™t let the new model deviate too far from the old one (via a KL penalty or clipping).

## ðŸ§© Step-by-Step Flow of RLHF with PPO

### **1ï¸âƒ£ Sample Prompts**

Select some input prompts (e.g., â€œExplain quantum computing in simple terms.â€)

### **2ï¸âƒ£ Generate Responses**

The **policy model (Ï€Î¸)** generates responses using sampling (temperature, top-p, etc.)

### **3ï¸âƒ£ Compute Rewards**

Each generated response is scored by the **Reward Model (RÏ•)** â†’ gives a scalar reward (e.g., +4.2).

### **4ï¸âƒ£ Add KL Penalty**

We penalize outputs that deviate too far from the **reference model (Ï€_ref)**.

    final reward = RÏ• - Î² Ã— KL(Ï€Î¸âˆ£âˆ£Ï€refâ€‹)

This keeps the new model close to its original behavior.

### **5ï¸âƒ£ Compute Advantages**

We estimate how much better each action (token generation) was compared to the baseline:

    A(t)â€‹ = reward - expected reward(value head output)

### **6ï¸âƒ£ PPO Optimization**

Use the **PPO loss** to update the modelâ€™s weights:

    L(PPO)â€‹ = min[r(t).â€‹A(t)â€‹, clip(r(t)â€‹, 1 âˆ’ Îµ, 1 + Îµ).A(t)â€‹]
    where where rð‘¡ = ðœ‹ðœƒ(að‘¡âˆ£sð‘¡) / ðœ‹ð‘œð‘™ð‘‘(að‘¡âˆ£sð‘¡)

This ensures **small, stable policy updates**.

## ðŸ§  Analogy

Imagine:

* The **SFT model** is a polite student.
* The **Reward Model** is a teacher grading responses.
* PPO is a training schedule where the student improves slowly, step by step, without changing personality.

## ðŸ“Š Benefits

âœ… Produces **aligned and helpful** models.
âœ… Prevents **reward hacking** (thanks to KL regularization).
âœ… Maintains **training stability** (via PPO clipping).
âœ… Generalizes well with diverse feedback datasets.

## ðŸ§¾ Summary Table

| Component                   | Role                      |
| --------------------------- | ------------------------- |
| **Policy (Ï€Î¸)**             | The LLM weâ€™re training    |
| **Reward Model (RÏ•)**       | Scores responses          |
| **Reference Model (Ï€_ref)** | Keeps behavior grounded   |
| **PPO Algorithm**           | Stabilizes the updates    |
| **KL Penalty**              | Prevents drift from SFT   |
| **Output**                  | Final human-aligned model |

---

## âš™ï¸ Formula Summary

Objective:max(Î¸) â€‹E[RÏ•âˆ’Î²KL(Ï€Î¸âˆ£âˆ£Ï€refâ€‹)]

Optimized using: PPO loss with clipping and advantage estimation.

### ðŸ”„ Full Loop

    Prompt â†’ Policy (LLM) â†’ Response â†’ Reward Model â†’ Reward Score
               â†“                                   â†‘
          PPO Update  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ KL Penalty â”€â”€â”˜

### ðŸ§© In Simple Words

> RLHF with PPO fine-tunes your LLM using *human feedback as a compass* and *PPO as a steering wheel*, so the model learns to be **more aligned, helpful, and stable** â€” without forgetting what it already knows.

---

Would you like me to follow up with a **visual diagram** of this RLHF + PPO loop (prompt â†’ policy â†’ reward â†’ PPO update)? It makes the flow much clearer.
